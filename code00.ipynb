{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio.transforms as T\n",
    "import torchaudio.functional as F\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_dir = '../../LibriVox_Kaggle/'\n",
    "bg_dir = '../../LibriVox_Kaggle/BGnoise/'\n",
    "rir_dir = '../../RIR/MIT_IR_Survey/Audio/'\n",
    "train_csv_file = 'only_audioFname_train.csv'\n",
    "test_csv_file = 'only_audioFname_test.csv'\n",
    "\n",
    "bg_files = os.listdir(bg_dir)\n",
    "rir_files = os.listdir(rir_dir)[1::]\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "N_FFT = 1024\n",
    "WIN_LEN = 1024\n",
    "HOP_LEN = 256\n",
    "\n",
    "spectrogram = T.Spectrogram(n_fft=N_FFT, \n",
    "                            win_length=WIN_LEN, \n",
    "                            hop_length=HOP_LEN, \n",
    "                            center=True, \n",
    "                            pad_mode=\"reflect\", \n",
    "                            power = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_audio(audio, sr):\n",
    "    resampled_audio = F.resample(audio, sr, SAMPLE_RATE)\n",
    "    return resampled_audio\n",
    "\n",
    "def stereo_to_mono(audio):\n",
    "    new_audio = torch.mean(audio, dim=0).unsqueeze(0)\n",
    "    return new_audio\n",
    "\n",
    "\n",
    "def load_audio(aud_fname):\n",
    "    \n",
    "    raw_wav, sampleRate = torchaudio.load(aud_fname)\n",
    "    if raw_wav.shape[0] == 2:\n",
    "        raw_wav = stereo_to_mono(raw_wav)\n",
    "    if sampleRate != SAMPLE_RATE:\n",
    "        raw_wav = resample_audio(raw_wav, sampleRate)\n",
    "    return raw_wav\n",
    "\n",
    "\n",
    "def add_noise(audio, rir, noise_wav, snr):\n",
    "    echo_audio = F.fftconvolve(audio, rir)[:,0:audio.shape[1]]\n",
    "    noisy_audio = F.add_noise(echo_audio, noise_wav[:,0:audio.shape[1]], torch.Tensor([snr]))\n",
    "    return noisy_audio\n",
    "\n",
    "def random_second_choice(audio):\n",
    "    duration = (int)(audio.shape[1]/SAMPLE_RATE)\n",
    "    random_sec = random.choice([i for i in range(0, duration-1)])\n",
    "    return random_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "\n",
    "    rir_fname = os.path.join(rir_dir,random.choice(rir_files))\n",
    "    bg_fname = os.path.join(bg_dir, random.choice(bg_files))\n",
    "    snr_choice = random.choice([5,10,20])\n",
    "\n",
    "    wav = load_audio(filename)\n",
    "    rir_ = load_audio(rir_fname)\n",
    "    bg = load_audio(bg_fname)\n",
    "\n",
    "    rand_wav_sec = random_second_choice(wav)\n",
    "    rand_bg_sec = random_second_choice(bg)\n",
    "\n",
    "    wav_sec = wav[:,rand_wav_sec*SAMPLE_RATE:(rand_wav_sec+1)*SAMPLE_RATE]\n",
    "    bg_sec = bg[:,rand_bg_sec*SAMPLE_RATE:(rand_bg_sec+1)*SAMPLE_RATE]\n",
    "\n",
    "    noisy_audio = add_noise(wav_sec, rir_, bg_sec, snr_choice)\n",
    "\n",
    "    return noisy_audio, bg_sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_inst = T.Spectrogram(\n",
    "    n_fft=400,\n",
    "    win_length=None,\n",
    "    hop_length=100,\n",
    "    power=None\n",
    ")\n",
    "\n",
    "inv_spec_inst = T.InverseSpectrogram(\n",
    "    n_fft=400,\n",
    "    win_length=None,\n",
    "    hop_length=100\n",
    ")\n",
    "\n",
    "def get_spectrogram(audio):\n",
    "    spec = T.Spectrogram(power=None)(audio)\n",
    "    return spec\n",
    "\n",
    "def get_audio_from_spectrogram(spec):\n",
    "    audio = T.InverseSpectrogram()(spec)\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec_image(audio):\n",
    "    \n",
    "    spec = spectrogram(audio.squeeze()).numpy()\n",
    "    fig, axs = plt.subplots()\n",
    "    plt.figure(figsize=(10,4))\n",
    "    img = axs.imshow(librosa.power_to_db(spec), interpolation=\"nearest\", origin=\"lower\", aspect=\"auto\", cmap=\"viridis\")\n",
    "    axs.axis('off')\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    pil_image = Image.fromarray(data)\n",
    "    plt.close(fig)\n",
    "    return pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 201, 81, 2]), torch.Size([1, 201, 81]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_aud = '../../LibriVox_Kaggle/achtgesichterambiwasse/achtgesichterambiwasse_0009.wav'\n",
    "\n",
    "noisy, noise = get_data(sample_aud)\n",
    "\n",
    "noisy_spec = get_spectrogram(noisy)\n",
    "noisy_aud = get_audio_from_spectrogram(noisy_spec)\n",
    "\n",
    "torch.view_as_real(noisy_spec).shape, noisy_spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, audio_csvfile, aud_dir):\n",
    "        self.audio_df = pd.read_csv(audio_csvfile)\n",
    "        self.aud_dir = aud_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = os.path.join(self.aud_dir, self.audio_df.iloc[index, 0])\n",
    "\n",
    "        audio_in, label = get_data(audio_path)\n",
    "        \n",
    "        audio_spec = get_spectrogram(audio_in)\n",
    "        label_spec = get_spectrogram(label)\n",
    "\n",
    "        return audio_spec, label_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "aud_dir = '../../LibriVox_Kaggle/'\n",
    "train_csv_file = 'only_audioFname_train.csv'\n",
    "test_csv_file = 'only_audioFname_test.csv'\n",
    "\n",
    "train_dataset = audioDataset('only_audioFname_train.csv', aud_dir)\n",
    "test_dataset = audioDataset('only_audioFname_test.csv', aud_dir)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 201, 81]), torch.Size([32, 1, 201, 81]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_spec, labs = next(iter(train_dataloader))\n",
    "audio_spec.shape, labs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "\n",
    "class speechRemoval00(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(speechRemoval00, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "\n",
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = speechRemoval00().to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::complex<float>) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/work/dpandya/giggityGit/speechRemoval/code00.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, inputs)\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/work/dpandya/giggityGit/speechRemoval/code00.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-07.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/code00.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::complex<float>) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "\n",
    "    loss_ten = torch.Tensor([])\n",
    "    for data in train_dataloader:\n",
    "        #print('pass')\n",
    "        model.train()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, inputs)\n",
    "\n",
    "        # BP and optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_ten = torch.cat((loss_ten,torch.Tensor([loss.item()])),0)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {torch.mean(loss_ten)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noiseremoval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
