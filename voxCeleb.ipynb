{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hf00\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display, Audio\n",
    "import torchaudio.transforms as T\n",
    "import librosa\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import convolve\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "speech_ = []\n",
    "noise_ = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas_dir = '../audioData/MUSAN/MUSAN/musan/'\n",
    "noise_dir = os.listdir(datas_dir+'noise')\n",
    "speech_dir = os.listdir(datas_dir+'speech')\n",
    "\n",
    "noise_files = []\n",
    "speech_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in noise_dir:\n",
    "    nois_dir = os.path.join((datas_dir+'noise'), i)\n",
    "    if os.path.isdir(nois_dir):\n",
    "        noise_files.extend(os.listdir(nois_dir))\n",
    "\n",
    "for i in speech_dir:\n",
    "    spech_dir = os.path.join((datas_dir+'speech'), i)\n",
    "    if os.path.isdir(spech_dir):\n",
    "        speech_files.extend(os.listdir(spech_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = []\n",
    "speech = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(930, 930, 426, 426)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "for i in range(0, len(noise_files)):\n",
    "    folder = noise_files[i].split('-')\n",
    "    #print(folder[1])\n",
    "    if folder[1] == 'free':\n",
    "        fname = datas_dir+'noise/free-sound/'+noise_files[i]\n",
    "        noise.append(fname)\n",
    "    else:\n",
    "        fname = datas_dir+'noise/sound-bible/'+noise_files[i]\n",
    "        noise.append(fname)\n",
    "\n",
    "for i in range(0, len(speech_files)):\n",
    "    folder = speech_files[i].split('-')\n",
    "    #print(folder[1])\n",
    "    if folder[1] == 'librivox':\n",
    "        fname = datas_dir+'speech/librivox/'+speech_files[i]\n",
    "        speech.append(fname)\n",
    "    else:\n",
    "        fname = datas_dir+'speech/us-gov/'+speech_files[i]\n",
    "        speech.append(fname)\n",
    "\n",
    "len(noise_files), len(noise), len(speech), len(speech_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "rir_dir = '../../RIR/MIT_IR_Survey/Audio/'\n",
    "rir_files = os.listdir(rir_dir)[1:]\n",
    "speech_ = speech\n",
    "noise_ = noise\n",
    "\n",
    "def get_seconds(audio):\n",
    "\n",
    "    duration = (int)(audio.shape[0]/SAMPLE_RATE)\n",
    "    audio_list = []\n",
    "    \n",
    "    for i in range(0, duration*SAMPLE_RATE, SAMPLE_RATE):\n",
    "        audio_list.append(audio[i:i+SAMPLE_RATE])\n",
    "    return audio_list\n",
    "\n",
    "def round_up_audio(audio):\n",
    "    \n",
    "    rem = audio.shape[0]%SAMPLE_RATE\n",
    "    zero_len = SAMPLE_RATE-rem\n",
    "    added_arr = np.zeros(zero_len, audio.dtype)\n",
    "    ext_audio = np.concatenate((audio, added_arr), axis=None)\n",
    "\n",
    "    return ext_audio\n",
    "\n",
    "def add_echo_from_file(filename, audio):\n",
    "\n",
    "    rir_wav,sr_rir = librosa.load(filename, sr=SAMPLE_RATE)\n",
    "    echo_audio = convolve(audio, rir_wav, mode='full')\n",
    "\n",
    "    return echo_audio[0:SAMPLE_RATE]\n",
    "\n",
    "def get_noise_from_sound(signal,noise,SNR):\n",
    "    \n",
    "    RMS_s=math.sqrt(np.mean(signal**2))\n",
    "    #required RMS of noise\n",
    "    RMS_n=math.sqrt(RMS_s**2/(pow(10,SNR/10)))\n",
    "    \n",
    "    #current RMS of noise\n",
    "    RMS_n_current=math.sqrt(np.mean(noise**2))\n",
    "    noise=noise*(RMS_n/RMS_n_current)\n",
    "    \n",
    "    return noise\n",
    "\n",
    "def add_noise(audio, noise):\n",
    "\n",
    "    SNR_list = [i for i in range(0,10)]\n",
    "    SNR_choice = random.choice(SNR_list)\n",
    "    \n",
    "    noise = get_noise_from_sound(audio, noise, SNR_choice)\n",
    "    noisy_audio = audio + noise\n",
    "    return noisy_audio, noise\n",
    "\n",
    "def random_sec(audio):\n",
    "\n",
    "    duration = (int)(audio.shape[0]/SAMPLE_RATE)\n",
    "    #print(audio.shape, duration)\n",
    "    if duration > 1:\n",
    "        sec_choice = random.choice([i for i in range(0,duration-1)])\n",
    "        rand_sec = audio[(sec_choice*SAMPLE_RATE):(sec_choice+1)*SAMPLE_RATE]\n",
    "    if duration == 0:\n",
    "        rand_sec = np.zeros((SAMPLE_RATE,))\n",
    "    else:\n",
    "        rand_sec = audio[0:SAMPLE_RATE]\n",
    "    return rand_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav, sr = librosa.load(noise_[15])\n",
    "\n",
    "rand = random_sec(wav)\n",
    "rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_audio_sec(audio_filename):\n",
    "    \n",
    "    # Choosing a random background and echo filename\n",
    "    bg1_file = random.choice(noise_)\n",
    "    bg2_file = random.choice(noise_)\n",
    "    rir_file = rir_dir + random.choice(rir_files)\n",
    "\n",
    "    # Extracting audio data\n",
    "    wav, sr = librosa.load(audio_filename, sr=SAMPLE_RATE)\n",
    "    bg1_wav,sr =librosa.load(bg1_file, sr=SAMPLE_RATE)\n",
    "    bg2_wav,sr =librosa.load(bg2_file, sr=SAMPLE_RATE)\n",
    "\n",
    "    # Randomising and normalising audio data\n",
    "    wav = round_up_audio(wav)\n",
    "    wav /= np.max(np.abs(wav), axis=0)\n",
    "    bg1_wav /= np.max(np.abs(bg1_wav), axis=0)\n",
    "    bg2_wav /= np.max(np.abs(bg2_wav), axis=0)\n",
    "\n",
    "    # Getting a random audio and bg second\n",
    "    rand_audio_sec = random_sec(wav)\n",
    "    bg1_random_wav = random_sec(bg1_wav)\n",
    "    bg2_random_wav = random_sec(bg2_wav)\n",
    "    \n",
    "    # Adding echo and bg noise to the audio\n",
    "    echo_audio = add_echo_from_file(rir_file, rand_audio_sec)\n",
    "    #print(echo_audio.shape, bg_wav.shape)\n",
    "    bg_random_wav = bg1_random_wav+bg2_random_wav\n",
    "    noisy_audio, noise_lab = add_noise(echo_audio, bg_random_wav)\n",
    "\n",
    "    noisy_audio = torch.from_numpy(noisy_audio).unsqueeze(0)\n",
    "    noise_label = torch.from_numpy(noise_lab).unsqueeze(0)\n",
    "\n",
    "    return noisy_audio, noise_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectro_fn = T.Spectrogram(power=None)\n",
    "inv_spectro_fn = T.InverseSpectrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class audioDataset(Dataset):\n",
    "\n",
    "    def __init__(self, speech_list):\n",
    "        #self.audio_df = pd.read_csv(audio_csvfile)\n",
    "        self.speech_list = speech_list\n",
    "        #self.audio_dir = audio_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.speech_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "    \n",
    "        audio_path = self.speech_list[index]\n",
    "        audio, label = get_random_audio_sec(audio_path)\n",
    "\n",
    "        audio_spec = spectro_fn(audio)\n",
    "        label_spec = spectro_fn(label)\n",
    "\n",
    "        return audio_spec.real, label_spec.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = audioDataset(speech_)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "\n",
    "class SpecAE(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SpecAE, self).__init__()\n",
    "        \n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=2,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=2,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(16, 8, kernel_size=3, stride=1,padding=1),\n",
    "            #nn.LeakyReLU()\n",
    "            )\n",
    "\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=3, stride=1,padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "    \n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.dec(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(13)\n",
    "torch.cuda.manual_seed(13)\n",
    "\n",
    "model = SpecAE()\n",
    "#model.load_state_dict(torch.load('specAEmodel01.pt'))\n",
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, inputs)\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menc(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdec(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdws-06.informatik.uni-mannheim.de/work/dpandya/giggityGit/speechRemoval/voxCeleb.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m/work/dpandya/miniconda3/envs/noiseremoval/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(0,epochs):\n",
    "    loss_list = []\n",
    "    for data in train_dataloader:\n",
    "        \n",
    "        model.train()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(outputs, inputs)\n",
    "\n",
    "        # BP and optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}] Loss: {statistics.mean(loss_list)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "noiseremoval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
